{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d9e212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf3ec27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "tensor0d = torch.tensor(1)\n",
    "tensor1d = torch.tensor([1,2,3])\n",
    "tensor2d = torch.tensor([[1,2],[3,4]])\n",
    "tensor3d = torch.tensor([[[1,2],[3,4]],[[5,6],[7,8]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7be921",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor1d = torch.tensor([1,2,3])\n",
    "print(tensor1d.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec47fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor1d = torch.tensor([1.0,2.0,3.0])\n",
    "print(tensor1d.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdef3113",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor1d = torch.tensor([1,2,3])\n",
    "print(tensor1d.dtype)\n",
    "floatvec = tensor1d.to(torch.float32)\n",
    "print(floatvec.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68923859",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor2d = torch.tensor([[1,2,3],[4,5,6]])\n",
    "print(tensor2d)\n",
    "print(tensor2d.shape)\n",
    "print(tensor2d.reshape(3,2))\n",
    "print(tensor2d.view(3,2))\n",
    "print(tensor2d.T)\n",
    "print(tensor2d.matmul(tensor2d.T)) #矩阵乘法\n",
    "print(tensor2d @ tensor2d.T) #矩阵乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c1801b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F #A 这是PyTorch中常见的导入约定,用于避免代码行过长\n",
    "\n",
    "y =  torch.tensor([1.0]) #B 真实标签\n",
    "x1 = torch.tensor([1.1]) #C 输入特征\n",
    "w1 = torch.tensor([2.2]) #D 权重参数\n",
    "b = torch.tensor([0.0]) #E 偏置单元\n",
    "z = x1 * w1 + b #F 网络输入\n",
    "a = torch.sigmoid(z) #G 激活与输出\n",
    "\n",
    "loss = F.binary_cross_entropy(a,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675dd62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([-0.0898]),)\n",
      "(tensor([-0.0817]),)\n",
      "tensor([-0.0898])\n",
      "tensor([-0.0817])\n"
     ]
    }
   ],
   "source": [
    "# requires_grad=True是一个开关： \n",
    "# 你可以告诉 PyTorch，对于某些参与计算的数字（更专业地说，是“张量”），\n",
    "# 你可能想知道它们是如何影响最终结果的。\n",
    "# 如果你这样做了（通过设置 requires_grad=True），\n",
    "# PyTorch 就会特别留意这些数字，并在计算图中记录下相关的操作。\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad\n",
    "\n",
    "y = torch.tensor([1.0])\n",
    "x1 = torch.tensor([1.1])\n",
    "w1 = torch.tensor([2.2], requires_grad=True)\n",
    "b = torch.tensor([0.0], requires_grad=True)\n",
    "\n",
    "z = x1 * w1 + b\n",
    "a = torch.sigmoid(z)\n",
    "\n",
    "loss = F.binary_cross_entropy(a, y)\n",
    "\n",
    "grad_L_w1 = grad(loss,w1,retain_graph=True)\n",
    "grad_L_b = grad(loss,b,retain_graph=True)\n",
    "\n",
    "# 默认情况下，PyTorch 在计算完梯度后会销毁计算图以释放内存。\n",
    "# 然而，由于我们稍后将重用这个计算图，所以我们设置了 retain_graph=True，使其保留在内存中。\n",
    "\n",
    "print(grad_L_w1)\n",
    "print(grad_L_b)\n",
    "\n",
    "# PyTorch 将计算图中所有叶节点的梯度，这些梯度将存储在张量的 .grad 属性中\n",
    "loss.backward()\n",
    "print(w1.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e9e45124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3113, 0.3934, 0.2952]])\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self,num_inputs,num_outputs): #A\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            #1st hidden layer\n",
    "            torch.nn.Linear(num_inputs,30), #B\n",
    "            torch.nn.ReLU(), #C\n",
    "\n",
    "            #2nd hidden layer\n",
    "            torch.nn.Linear(30,20), #D\n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            #output layer\n",
    "            torch.nn.Linear(20,num_outputs),\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        logits = self.layers(x) #E\n",
    "        return logits\n",
    "    \n",
    "#A 将输入和输出的数量编码为变量很有用，这样可以为具有不同特征和类别数量的数据集重用相同的代码。\n",
    "#B Linear 层将输入和输出节点的数量作为参数。\n",
    "#C 非线性激活函数放置在隐藏层之间。\n",
    "#D 一个隐藏层的输出节点数必须与下一个隐藏层的输入节点数相匹配。\n",
    "#E 最后一层的输出被称为 logits。\n",
    "\n",
    "# model  = NeuralNetwork(50,3) #实例化一个新的神经网络对象\n",
    "# print(model)\n",
    "\n",
    "# # 检查模型的总可训练参数数量\n",
    "# num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "# print(\"Total number of trainable model parameters:\", num_params)\n",
    "\n",
    "# # 第一个Linear层位于layers属性的索引位置0。\n",
    "# # 我们可以按如下方式访问相应的权重参数矩阵\n",
    "# print(model.layers[0].weight)\n",
    "# print(model.layers[0].weight.shape)\n",
    "# print(model.layers[0].bias)\n",
    "\n",
    "# #如果希望每次运行代码时，这些随机数都是一样的，这样方便我们做实验和调试。\n",
    "# #PyTorch提供了一个方法来实现这个目标，\n",
    "# # 我们可以通过使用 manual_seed 来为 PyTorch 的随机数生成器设置种子，\n",
    "# # 从而使随机数初始化可复现\n",
    "# torch.manual_seed(123)\n",
    "# model = NeuralNetwork(50, 3)\n",
    "# print(model.layers[0].weight)\n",
    "\n",
    "# #前项传播结果\n",
    "# torch.manual_seed(123)\n",
    "# X = torch.rand((1, 50))\n",
    "# out = model(X)\n",
    "# print(out)\n",
    "\n",
    "# # 当我们将模型用于推理（例如，进行预测）而不是训练时，\n",
    "# # 最佳实践是使用torch.no_grad()上下文管理器\n",
    "# with torch.no_grad():\n",
    "#     out = model(X)\n",
    "# print(out)\n",
    "\n",
    "# 在 PyTorch 中，一种常见的做法是将模型编码成返回最后一层（logits）的输出，\n",
    "# 而不会将它们传递给非线性激活函数。\n",
    "# 这是因为PyTorch 常用的损失函数将 softmax（或二元分类的sigmoid）运算与负对数似然损失组合在一个类中。\n",
    "# 这样做的原因是出于数值效率和稳定性的考虑。\n",
    "# 因此，如果我们想计算预测的类别成员概率，则必须显式调用softmax函数\n",
    "with torch.no_grad():\n",
    "    out = torch.softmax(model(X),dim=1)\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fj529",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
